{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = datasets.MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9492908f276430087128a10e372239f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ffcb7a7c194d9fb86be8d414968fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516e7c9984384ad185739678965a74da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dce8f31ab564f8abd90b2e3ee572650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Birgit\\Anaconda3\\envs\\Py3_ICNS_keras\\lib\\site-packages\\torchvision\\datasets\\mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist(\"data\", train=True, download=True, transform=T.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "const = datasets.ImageFolder(root=\"data\", transform=T.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 48\n",
       "    Root location: data\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.utils.data.dataset.Subset at 0x1ad85997048>,\n",
       " <torch.utils.data.dataset.Subset at 0x1ad85997088>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_split(const, [45, 3], generator=torch.Generator().manual_seed(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python compute_centroids.py --dataset constellations --num_clusters=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation sanity check: 0it [00:00, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | gpt       | GPT2             | 39.3 K\n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "39.3 K    Trainable params\n",
      "24        Non-trainable params\n",
      "39.3 K    Total params\n",
      "0.157     Total estimated model params size (MB)\n",
      "Traceback (most recent call last):\n",
      "  File \"run.py\", line 91, in <module>\n",
      "    args.func(args)\n",
      "  File \"run.py\", line 60, in train\n",
      "    trainer.fit(model, train_dl, valid_dl)\n",
      "  File \"C:\\Users\\Birgit\\Anaconda3\\envs\\Py3_ICNS_keras\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 499, in fit\n",
      "    self.dispatch()\n",
      "  File \"C:\\Users\\Birgit\\Anaconda3\\envs\\Py3_ICNS_keras\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 546, in dispatch\n",
      "    self.accelerator.start_training(self)\n",
      "  File \"C:\\Users\\Birgit\\Anaconda3\\envs\\Py3_ICNS_keras\\lib\\site-packages\\pytorch_lightning\\accelerators\\accelerator.py\", line 73, in start_training\n",
      "    self.training_type_plugin.start_training(trainer)\n",
      "  File \"C:\\Users\\Birgit\\Anaconda3\\envs\\Py3_ICNS_keras\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py\", line 114, in start_training\n",
      "    self._results = trainer.run_train()\n",
      "  File \"C:\\Users\\Birgit\\Anaconda3\\envs\\Py3_ICNS_keras\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 607, in run_train\n",
      "    self.run_sanity_check(self.lightning_module)\n",
      "  File \"C:\\Users\\Birgit\\Anaconda3\\envs\\Py3_ICNS_keras\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 864, in run_sanity_check\n",
      "    _, eval_results = self.run_evaluation(max_batches=self.num_sanity_val_batches)\n",
      "  File \"C:\\Users\\Birgit\\Anaconda3\\envs\\Py3_ICNS_keras\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 726, in run_evaluation\n",
      "    output = self.evaluation_loop.evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"C:\\Users\\Birgit\\Anaconda3\\envs\\Py3_ICNS_keras\\lib\\site-packages\\pytorch_lightning\\trainer\\evaluation_loop.py\", line 166, in evaluation_step\n",
      "    output = self.trainer.accelerator.validation_step(args)\n",
      "  File \"C:\\Users\\Birgit\\Anaconda3\\envs\\Py3_ICNS_keras\\lib\\site-packages\\pytorch_lightning\\accelerators\\accelerator.py\", line 177, in validation_step\n",
      "    return self.training_type_plugin.validation_step(*args)\n",
      "  File \"C:\\Users\\Birgit\\Anaconda3\\envs\\Py3_ICNS_keras\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py\", line 131, in validation_step\n",
      "    return self.lightning_module.validation_step(*args, **kwargs)\n",
      "  File \"C:\\Users\\Birgit\\Documents\\tehisnärvivõrgud\\constellation-recognition\\igpt\\image_gpt.py\", line 125, in validation_step\n",
      "    logits = self.gpt(x)\n",
      "  File \"C:\\Users\\Birgit\\Anaconda3\\envs\\Py3_ICNS_keras\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Birgit\\Documents\\tehisnärvivõrgud\\constellation-recognition\\igpt\\gpt2.py\", line 70, in forward\n",
      "    h = h + self.position_embeddings(positions).expand_as(h)\n",
      "  File \"C:\\Users\\Birgit\\Anaconda3\\envs\\Py3_ICNS_keras\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Birgit\\Anaconda3\\envs\\Py3_ICNS_keras\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\", line 158, in forward\n",
      "    self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "  File \"C:\\Users\\Birgit\\Anaconda3\\envs\\Py3_ICNS_keras\\lib\\site-packages\\torch\\nn\\functional.py\", line 1916, in embedding\n",
      "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "IndexError: index out of range in self\n"
     ]
    }
   ],
   "source": [
    "!python run.py --dataset constellations train configs/xxs_gen.yml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
